@article{valvoda+al.tacl22, 
  title = {On the Role of Negative Precedent in Legal Outcome Prediction},
  venue = {TACL},
  year = {2023},
  slides = {https://docs.google.com/presentation/d/1q9sca-5lVarkZckDcUMqp_SGCSPlhw5ftfprXiwxQvo/edit?usp=sharing},
  code = {https://github.com/valvoda/Negative-Precedent-in-Legal-Outcome-Prediction},
  arXiv = {https://arxiv.org/abs/2208.08225},
  author = {Valvoda, Josef and 
	Teufel, Simone and 
	Cotterell, Ryan},
  booktitle = {Transactions of the Association for Computational Linguistics},
  month = {January},
  publisher = {Association for Computational Linguistics},
  volume = {11},
  pages = {34-48},
  abstract = {Every legal case sets a precedent by devel-
oping the law in one of the following two
ways. It either expands its scope, in which
case it sets positive precedent, or it narrows
it down, in which case it sets negative
precedent. While legal outcome prediction,
which is nothing other than the prediction of
positive precedents, is an increasingly pop-
ular task in AI, we are the first to investigate
negative precedent prediction by focusing
on negative outcomes. We discover an
asymmetry in existing modelsâ€™ ability to
predict positive and negative outcomes.
Where state-of-the-art outcome prediction
models predicts positive outcomes at
75.06 F1, they predicts negative outcomes
at only 10.09 F1, worse than a random
baseline. To address this performance
gap, we develop two new models inspired
by the dynamics of a court process. Our
first model significantly improves positive
outcome prediction score to 77.15 F1 and
our second model more than doubles the
negative outcome prediction performance
to 24.01 F1. Despite this improvement,
shifting focus to negative outcomes reveals
that there is still plenty of room to grow
when it comes to modelling law.},
  url = {https://arxiv.org/abs/2208.08225},
}
