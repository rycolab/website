@inproceedings{li+al.naacl22, 
  title = {Probing via Prompting},
  venue = {NAACL},
  year = {2022},
  slides = {https://docs.google.com/presentation/d/1i9BHYMBoT7icMOs46QMFCcVY6plpSH1mEs55BNHq5VQ/edit?usp=sharing},
  video = {https://www.youtube.com/watch?v=KVQgntbtIgA&t=26s},
  code = {https://github.com/rycolab/probing-via-prompting},
  arXiv = {https://arxiv.org/abs/2207.01736},
  author = {Li, Jiaoda and 
	Cotterell, Ryan and 
	Sachan, Mrinmaya},
  booktitle = {Proceedings of the 2022 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month = {July},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  volume = {1 (Long and Short Papers)},
  pages = {1144–1157},
  abstract = {Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting task. We conduct experiments on five probing tasks and show that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own. We further combine the probing via prompting approach with pruning to analyze where the model stores the linguistic information in its architecture. Finally, we apply the probing via prompting approach to examine the usefulness of a linguistic property for pre-training by removing the heads that are essential to it and evaluating the resulting model’s performance on language modeling.},
  url = {https://arxiv.org/abs/2207.01736},
}
