@inproceedings{cotterell+al.conll17, 
  title = {{CoNLL}--{SIGMORPHON} 2017 Shared Task: {U}niversal Morphological Reinflection in 52 Languages},
  venue = {CoNLL},
  year = {2017},
  anthology = {https://www.aclweb.org/anthology/K17-2001.pdf},
  author = {Cotterell, Ryan and 
	Kirov, Christo and 
	Sylak-Glassman, John and 
	Walther, Géraldine and 
	Vylomova, Ekaterina and 
	Xia, Patrick and 
	Faruqui, Manaal and 
	Kübler, Sandra and 
	Yarowsky, David and 
	Eisner, Jason and 
	Hulden, Mans},
  booktitle = {Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection},
  month = {August},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  pages = {1--30},
  abstract = {The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in non-identical sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.},
  url = {https://www.aclweb.org/anthology/K17-2001.pdf},
}
