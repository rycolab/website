@inproceedings{jiang+al.naacl22, 
  title = {{BlonDe}: An Automatic Evaluation Metric for Document-level Machine Translation},
  venue = {NAACL},
  year = {2022},
  slides = {https://docs.google.com/presentation/d/1NHZTz_bgNsRaOl4froVaH2TEedTx7qBOO0ZyEo-qxLY/edit?usp=sharing},
  video = {https://www.youtube.com/watch?v=axyzgXUf8Ps&feature=youtu.be},
  code = {https://github.com/EleanorJiang/blonde},
  arXiv = {https://arxiv.org/abs/2103.11878},
  author = {Eleanor Jiang, Yuchen and 
	Liu, Tianyu and 
	Ma, Shuming and 
	Zhang, Dongdong and 
	Yang, Jian and 
	Huang, Haoyang and 
	Sennrich, Rico and 
	Cotterell, Ryan and 
	Sachan, Mrinmaya and 
	Zhou, Ming},
  booktitle = {Proceedings of the 2022 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month = {July},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  volume = {1 (Long and Short Papers)},
  pages = {1550--1565},
  abstract = {Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones, nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans and calculating the similarity-based F1 measure of categorized spans. We conduct extensive comparisons on a newly constructed dataset BWB. The experimental results show that BlonDe possesses better selectivity and interpretability at the document-level, and is more sensitive to document-level nuances. In a large-scale human study, BlonDe also achieves significantly higher Pearsonâ€™s r correlation with human judgments compared to previous metrics.},
  url = {https://arxiv.org/abs/2103.11878},
}
