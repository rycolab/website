@inproceedings{opedal+al.icml24, 
  title = {Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?},
  venue = {ICML},
  year = {2024},
  slides = {N/A},
  video = {N/A},
  code = {https://github.com/eth-lre/solving-biases},
  arXiv = {https://arxiv.org/pdf/2401.18070},
  author = {Opedal, Andreas and 
	Stolfo, Alessandro and 
	Shirakami, Haruki and 
	Jiao, Ying and 
	Cotterell, Ryan and 
	Sch√∂lkopf, Bernhard and 
	Saparov, Abulhair and 
	Sachan, Mrinmaya},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  month = {july},
  publisher = {International Conference on Machine Learning (ICML)},
  address = {Vienna, Austria},
  abstract = {There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive  biases as children in these steps. We generate a novel set of word  problems for each of these tests, using a neuro-symbolic approach that  enables fine-grained control over the problem features. We find evidence  that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of  the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.},
  url = {https://arxiv.org/pdf/2401.18070},
}
