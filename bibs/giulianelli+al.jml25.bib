@inproceedings{giulianelli+al.jml25, 
  title = {Incremental Alternative Sampling as a Lens into the Temporal and Representational Resolution of Linguistic Prediction},
  venue = {JML},
  year = {2025},
  slides = {N/A},
  video = {N/A},
  code = {https://github.com/glnmario/ias-lang-comp/},
  arXiv = {https://osf.io/preprints/psyarxiv/fhp84},
  author = {Giulianelli, Mario and 
	Wallbridge, Sarenne and 
	Cotterell, Ryan and 
	Fernández, Raquel},
  booktitle = {PsyArXiv},
  abstract = {This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS). Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alternatives. Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender's predicted continuations of partial linguistic stimuli thanks to (i) an expressive representation function that captures different levels of linguistic processing and (ii) the bootstrapping of long-horizon prediction error. Our results show that IAS estimates of processing difficulty, computed with autoregressive language models via Monte Carlo estimation, have greater predictive power than surprisal extracted from the same language models for most neural and behavioural responses under analysis—including reading times, event-related brain potentials, cloze and predictability judgements. Perhaps more importantly, IAS estimates provide insight into the nature of the predictive mechanisms that generate those responses during language comprehension. Variability in neural and behavioural responses is well explained by different combinations of the representational and temporal resolution of prediction. Processing difficulty calculated at varying representational domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are determined by a combination of experimental task setup and naturalness of the stimulus. Beyond enriching psycholinguistic models, IAS can also provide insights into the information processing mechanisms of computational language models. Our analysis of next-word surprisal under the lenses of IAS reveals that, despite the metric's seemingly narrow focus on the upcoming word, language model surprisal implicitly captures anticipatory processing of multiple future lexical items.},
  url = {https://osf.io/preprints/psyarxiv/fhp84},
}
