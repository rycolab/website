@inproceedings{immer+al.acl22, 
  title = {Probing as Quantifying the Inductive Bias of Pre-trained Representations},
  venue = {ACL},
  year = {2022},
  video = {https://www.youtube.com/watch?v=zn3qK4YMR-c},
  code = {https://github.com/rycolab/evidence-probing},
  arXiv = {https://arxiv.org/abs/2110.08388},
  author = {Immer, Alexander and 
	Torroba Hennigen, Lucas and 
	Fortuin, Vincent and 
	Cotterell, Ryan},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month = {May},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  volume = {1 (Long Papers)},
  pages = {1839--1851},
  abstract = {Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks. Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations. In general, researchers quantify the amount of linguistic information through probing, an endeavor which consists of training a supervised model to predict a linguistic property directly from the contextual representations. Unfortunately, this definition of probing has been subject to extensive criticism in the literature, and has been observed to lead to paradoxical and counter-intuitive results. In the theoretical portion of this paper, we take the position that the goal of probing ought to be measuring the amount of inductive bias that the representations encode on a specific task. We further describe a Bayesian framework that operationalizes this goal and allows us to quantify the representations’ inductive bias. In the empirical portion of the paper, we apply our framework to a variety of NLP tasks. Our results suggest that our proposed framework alleviates many previous problems found in probing. Moreover, we are able to offer concrete evidence that—for some tasks—fastText can offer a better inductive bias than BERT.},
  url = {https://arxiv.org/abs/2110.08388},
}
