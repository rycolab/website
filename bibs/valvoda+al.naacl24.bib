@inproceedings{valvoda+al.naacl24, 
  title = {Towards Explainability in Legal Outcome Prediction Models},
  venue = {NAACL},
  year = {2024},
  slides = {N/A},
  video = {N/A},
  code = {https://github.com/valvoda/under_the_influence},
  arXiv = {https://arxiv.org/pdf/2403.16852},
  author = {Valvoda, Josef and 
	Cotterell, Ryan},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  volume = {Volume 1: Long Papers},
  pages = {7269â€“7289},
  abstract = {Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand the model's decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and neural models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.},
  url = {https://arxiv.org/pdf/2403.16852},
}
