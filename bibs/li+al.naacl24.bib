@inproceedings{li+al.naacl24, 
  title = {A Transformer with Stack Attention},
  venue = {NAACL Findings},
  year = {2024},
  slides = {N/A},
  video = {N/A},
  code = {https://github.com/rycolab/stack-transformer},
  arXiv = {https://arxiv.org/abs/2405.04515},
  author = {Li, Jiaoda and 
	C. White, Jennifer and 
	Sachan, Mrinmaya and 
	Cotterell, Ryan},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  pages = {4318â€“4335},
  abstract = {Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-basedattention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-freelanguages.},
  url = {https://arxiv.org/abs/2405.04515},
}
