@inproceedings{cotterell+al.naacl18a, 
  title = {Are All Languages Equally Hard to Language-Model?},
  venue = {NAACL},
  year = {2018},
  arXiv = {https://arxiv.org/abs/1806.03743},
  anthology = {https://www.aclweb.org/anthology/papers/N18-2085.pdf},
  author = {Cotterell, Ryan and 
	Mielke, Sabrina J. and 
	Eisner, Jason and 
	Roark, Brian},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month = {June},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  volume = {2 (Short Papers)},
  pages = {536--541},
  abstract = {For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.},
  url = {https://www.aclweb.org/anthology/papers/N18-2085.pdf},
}
