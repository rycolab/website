+++
# A Featured Publications section created with the Featured Content widget.
# This section displays publications from `content/publication/` which have
# `featured = true` in their front matter.

widget = "blank"
headless = false  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 50  # Order that this section will appear.
title = "Teaching"
subtitle = "[SEE ALL CLASSES](/classes)"

[content]
  # Page type to display. E.g. post, talk, or publication.
  page_type = "classes"
  
  # Choose how much pages you would like to display (0 = all pages)
  count = 0

  # Page order. Descending (desc) or ascending (asc) date.
  order = "desc"


  # Filter posts by a taxonomy term.
  [content.filters]
    tag = ""
    category = ""
    publication_type = ""

[design]
  # Toggle between the various page layout types.
  #   1 = List
  #   2 = Compact
  #   3 = Card
  #   4 = Citation (publication only)
  view = 2
  columns = "2"
  
[design.background]
  # Apply a background color, gradient, or image.
  #   Uncomment (by removing `#`) an option to apply it.
  #   Choose a light or dark text color by setting `text_color_light`.
  #   Any HTML color name or Hex value is valid.

  # Background color.
  # color = "#f0fff3"
  
  # Background gradient.
  # gradient_start = "DeepSkyBlue"
  # gradient_end = "SkyBlue"
  

  # Text color (true=light or false=dark).
  text_color_light = false

  
[advanced]
 # Custom CSS. 
 css_style = ""
 
 # CSS class.
 css_class = ""

[blackfriday]
  fractions = false
+++

## [Advanced Formal Language Theory](/classes/aflt-s25) 
**ETH Zürich** <span class="middot-divider"></span> **Spring 2025**
This course serves as an introduction to various advanced topics in formal language theory. The primary focus of the course is on weighted formalisms, which can easily be applied in machine learning. Topics include finite-state machines as well as the algorithms that are commonly used for their manipulation. We will also cover weighted context-free grammars, weighted pushdown automata, weighted tree automata, and weighted mildly context-sensitive formalisms.

## [NLP in the Wild](classes/nlp-bachelor-seminar-s25)
**ETH Zürich** <span class="middot-divider"></span> **Spring 2025**
In recent years, NLP has become a part of our daily lives. Many of us use chatbots like ChatGPT or Claude to translate a text from a foreign language, help draft essays, write code, and even answer everyday-life questions. However, even though most people recognize the utility of such tools, there are still many questions to be answered about their reliability and their impact on society. In this bachelor seminar, we take a closer look at NLP by discussing some of the most influential NLP papers and opinion articles, with a special focus on controversial topics regarding the role of NLP in society. Each week, students will present a selected paper, followed by a class discussion.

## [Natural Language Processing](classes/intro-nlp-f24)
**ETH Zürich** <span class="middot-divider"></span> **Fall 2024**
This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.

## [Neural Networks and Computational Complexity](classes/nlp-bachelor-seminar-f24)
**ETH Zürich** <span class="middot-divider"></span> **Fall 2024**
This Bachelor’s seminar delves into the fascinating world of modern large language models (LLMs), which have revolutionized natural language processing. As these models continue to evolve and impact various domains, we will explore their potential, limitations, and underlying mechanisms through a theroretical lens. Throughout the seminar, we will address the following key questions: what are the real capabilities of large language models? What are their inherent limitations? How do these models function at a fundamental level? Under what circumstances are they likely to fail? Can we develop a comprehensive "science of LLMs" to address these inquiries? We will leverage formal language theory to provide a rigorous framework for understanding the representational capacity of neural language models.

## [NLP in the Wild](classes/nlp-bachelor-seminar-s24)
**ETH Zürich** <span class="middot-divider"></span> **Spring 2024**
In recent years, NLP has become a part of our daily lives. Many of us use tools like Google Translate to understand sentences in languages we don’t know,  and chatbots like ChatGPT to help draft essays and answer basic questions. However, even though most people recognize the utility of such tools, there are still many questions to be answered about their reliability and their impact on society. For example, to what extent can we or should we trust what ChatGPT says? Should chatbots ever be used in legal decision-making? What is the role that NLP should play in the education system? In this open-ended seminar, we will read and discuss opinions on the proper use of NLP in the real world, or as we term it,  NLP in the wild!

## [Large Language Models](/classes/llm-s25) 
**ETH Zürich** <span class="middot-divider"></span> **Spring 2025**
Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. In this course, we start with the probabilistic foundations of language models, i.e., covering what constitutes a language model from a formal, theoretical perspective. We then discuss how to construct and curate training corpora, and introduce many of the neural-network architectures often used to instantiate language models at scale. The course discusses privacy and harms, as well as applications of language models in NLP and beyond.

## [Philosophy of Language and Computation I](/classes/phil-s24) 
**ETH Zürich** <span class="middot-divider"></span> **Spring 2024**
This graduate class, partly taught like a seminar, is designed to help you understand the philosophical underpinnings of modern work in natural language processing (NLP), most of which centered around statistical machine learning applied to natural language data.

## [Generating Text from Language Models](/classes/acl-2023-tutorial) 
**ACL (Toronto)** <span class="middot-divider"></span> **July 2023**
In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate text from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties.

## [Formal Language Theory and Neural Networks](/classes/esslli-23) 
**ESSLLI (Ljubljana, Slovenia)** <span class="middot-divider"></span> **Spring 2023**
