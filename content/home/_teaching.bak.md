
## [Neural Networks and Computational Complexity](classes/nlp-bachelor-seminar-f24)
**ETH Zürich** <span class="middot-divider"></span> **Fall 2024**
This Bachelor’s seminar delves into the fascinating world of modern large language models (LLMs), which have revolutionized natural language processing. As these models continue to evolve and impact various domains, we will explore their potential, limitations, and underlying mechanisms through a theroretical lens. Throughout the seminar, we will address the following key questions: what are the real capabilities of large language models? What are their inherent limitations? How do these models function at a fundamental level? Under what circumstances are they likely to fail? Can we develop a comprehensive "science of LLMs" to address these inquiries? We will leverage formal language theory to provide a rigorous framework for understanding the representational capacity of neural language models.

## [NLP in the Wild](classes/nlp-bachelor-seminar-s24)
**ETH Zürich** <span class="middot-divider"></span> **Spring 2024**
In recent years, NLP has become a part of our daily lives. Many of us use tools like Google Translate to understand sentences in languages we don’t know,  and chatbots like ChatGPT to help draft essays and answer basic questions. However, even though most people recognize the utility of such tools, there are still many questions to be answered about their reliability and their impact on society. For example, to what extent can we or should we trust what ChatGPT says? Should chatbots ever be used in legal decision-making? What is the role that NLP should play in the education system? In this open-ended seminar, we will read and discuss opinions on the proper use of NLP in the real world, or as we term it,  NLP in the wild!

## [Large Language Models](/classes/llm-s25) 
**ETH Zürich** <span class="middot-divider"></span> **Spring 2025**
Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. In this course, we start with the probabilistic foundations of language models, i.e., covering what constitutes a language model from a formal, theoretical perspective. We then discuss how to construct and curate training corpora, and introduce many of the neural-network architectures often used to instantiate language models at scale. The course discusses privacy and harms, as well as applications of language models in NLP and beyond.

## [Philosophy of Language and Computation I](/classes/phil-s24) 
**ETH Zürich** <span class="middot-divider"></span> **Spring 2024**
This graduate class, partly taught like a seminar, is designed to help you understand the philosophical underpinnings of modern work in natural language processing (NLP), most of which centered around statistical machine learning applied to natural language data.

## [Generating Text from Language Models](/classes/acl-2023-tutorial) 
**ACL (Toronto)** <span class="middot-divider"></span> **July 2023**
In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate text from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties.

## [Formal Language Theory and Neural Networks](/classes/esslli-23) 
**ESSLLI (Ljubljana, Slovenia)** <span class="middot-divider"></span> **Spring 2023**
