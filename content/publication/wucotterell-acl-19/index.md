---
title: "Exact Hard Monotonic Attention for Character-Level Transduction"
date: 2019-07-01
publishDate: 2021-05-05T10:12:37.988706Z
authors: ["Shijie Wu", "Ryan Cotterell"]
publication_types: ["1"]
abstract: "Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer."
featured: false
publication: "*Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*"
publication_short: "ACL"
links:
- name: Anthology
  url: https://www.aclweb.org/anthology/P19-1148.pdf
- name: arXiv
  url: https://arxiv.org/pdf/1905.06319
url_pdf: papers/wu+cotterell.acl19.pdf
---

