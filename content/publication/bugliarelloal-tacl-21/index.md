---
title: 'Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of
  Vision-and-Language BERTs'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
- Emanuele Bugliarello
- Ryan Cotterell
- Naoaki Okazaki
- Desmond Elliott

# Author notes (such as 'Equal Contribution')
author_notes: []

date: '2021-01-01'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2026-02-28T10:54:56.506124Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types:
- '2'

# Publication name and optional abbreviated publication name.
publication: '*Transactions of the Association for Computational Linguistics*'
publication_short: ''

abstract: Large-scale pretraining and task-specific fine-tuning is now the standard
  methodology for many tasks in computer vision and natural language processing. Recently,
  a multitude of methods have been proposed for pretraining vision and language BERTs
  to tackle challenges at the intersection of these two key areas of AI. These models
  can be categorized into either single-stream or dual-stream encoders. We study the
  differences between these two categories, and show how they can be unified under
  a single theoretical framework. We then conduct controlled experiments to discern
  the empirical differences between five vision and language BERTs. Our experiments
  show that training data and hyperparameters are responsible for most of the differences
  between the reported results, but they also reveal that the embedding layer plays
  a crucial role in these massive models.

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
venue: TACL
links:
- name: URL
  url: https://arxiv.org/abs/2011.15124
---

Add the **full text** or **supplementary notes** for the publication here using Markdown formatting.
