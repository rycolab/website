---
# Documentation: https://wowchemy.com/docs/managing-content/

title: On the Role of Negative Precedent in Legal Outcome Prediction
subtitle: ''
summary: ''
authors:
- Josef Valvoda
- Simone Teufel
- Ryan Cotterell
tags: []
categories: []
date: '2022-01-01'
lastmod: 2022-11-21T00:29:32+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-07-09T14:51:09.180668Z'
publication_types:
- '2'
abstract: Every legal case sets a precedent by devel- oping the law in one of the
  following two ways. It either expands its scope, in which case it sets positive
  precedent, or it narrows it down, in which case it sets negative precedent. While
  legal outcome prediction, which is nothing other than the prediction of positive
  precedents, is an increasingly pop- ular task in AI, we are the first to investigate
  negative precedent prediction by focusing on negative outcomes. We discover an asymmetry
  in existing modelsâ€™ ability to predict positive and negative outcomes. Where state-of-the-art
  outcome prediction models predicts positive outcomes at 75.06 F1, they predicts
  negative outcomes at only 10.09 F1, worse than a random baseline. To address this
  performance gap, we develop two new models inspired by the dynamics of a court process.
  Our first model significantly improves positive outcome prediction score to 77.15
  F1 and our second model more than doubles the negative outcome prediction performance
  to 24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals
  that there is still plenty of room to grow when it comes to modelling law.
publication: '*Transactions of the Association for Computational Linguistics*'
url_pdf: papers/valvoda+al.tacl22.pdf
---
