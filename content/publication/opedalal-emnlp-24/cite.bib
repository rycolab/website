@inproceedings{opedal+al.emnlp24,
 abstract = {We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.},
 address = {Miami, United States},
 arxiv = {https://arxiv.org/abs/2409.08160},
 author = {Opedal, Andreas and 
Chodroff, Eleanor and 
Cotterell, Ryan and 
Gotlieb Wilcox, Ethan},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
 code = {https://github.com/rycolab/context-reading-time},
 month = {November},
 pages = {3042â€“3058},
 publisher = {Association for Computational Linguistics},
 slides = {N/A},
 title = {On The Role of Context in Reading Time Prediction},
 url = {https://arxiv.org/abs/2409.08160},
 venue = {EMNLP},
 video = {N/A},
 year = {2024}
}
