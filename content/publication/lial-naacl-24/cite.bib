@inproceedings{li+al.naacl24,
 abstract = {Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-basedattention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-freelanguages.},
 address = {Mexico City, Mexico},
 arxiv = {https://arxiv.org/abs/2405.04515},
 author = {Li, Jiaoda and 
C. White, Jennifer and 
Sachan, Mrinmaya and 
Cotterell, Ryan},
 booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
 code = {https://github.com/rycolab/stack-transformer},
 pages = {4318â€“4335},
 publisher = {Association for Computational Linguistics},
 slides = {N/A},
 title = {A Transformer with Stack Attention},
 url = {https://arxiv.org/abs/2405.04515},
 venue = {NAACL Findings},
 video = {N/A},
 year = {2024}
}
