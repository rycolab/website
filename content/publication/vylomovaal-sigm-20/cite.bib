@inproceedings{vylomova+al.sigm20,
 title = {SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection},
 author = {Vylomova, Ekaterina and 
White, Jennifer and 
Salesky, Elizabeth and 
J. Mielke, Sabrina and 
Wu, Shijie and 
Maria Ponti, Edoardo and 
Hall Maudslay, Rowan and 
Zmigrod, Ran and 
Valvoda, Josef and 
Toldova, Svetlana and 
Tyers, Francis and 
Klyachko, Elena and 
Yegorov, Ilya and 
Krizhanovsky, Natalia and 
Czarnowska, Paula and 
Nikkarinen, Irene and 
Krizhanovsky, Andrew and 
Pimentel, Tiago and 
Torroba Hennigen, Lucas and 
Kirov, Christo and 
Nicolai, Garrett and 
Williams, Adina and 
Anastasopoulos, Antonios and 
Cruz, Hilaria and 
Chodroff, Eleanor and 
Cotterell, Ryan and 
Silfverberg, Miikka and 
Hulden, Mans},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 month = {July},
 year = {2020},
 address = {Online},
 publisher = {Association for Computational Linguistics},
 pages = {1--39},
 url = {https://www.aclweb.org/anthology/2020.sigmorphon-1.1/},
 arxiv = {https://arxiv.org/abs/2006.11572},
 abstract = {A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systemsâ€™ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.}
}

