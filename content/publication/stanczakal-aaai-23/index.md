---
# Documentation: https://wowchemy.com/docs/managing-content/

title: A Latent-Variable Model for Intrinsic Probing
subtitle: ''
summary: ''
authors:
- Karolina Sta≈Ñczak
- Lucas Torroba Hennigen
- Adina Williams
- Ryan Cotterell
- Isabelle Augenstein
tags: []
categories: []
date: '2023-01-01'
lastmod: 2023-07-07T16:00:00+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2024-02-28T20:04:38.045438Z'
publication_types:
- '1'
abstract: The success of pre-trained contextualized representations has prompted researchers
  to analyze them for the presence of linguistic information. Indeed, it is natural
  to assume that these pre-trained representations do encode some level of linguistic
  knowledge as they have brought about large empirical improvements on a wide variety
  of NLP tasks, which suggests they are learning true linguistic generalization. In
  this work, we focus on intrinsic probing, an analysis technique where the goal is
  not only to identify whether a representation encodes a linguistic attribute but
  also to pinpoint where this attribute is encoded. We propose a novel latent-variable
  formulation for constructing intrinsic probes and derive a tractable variational
  approximation to the log-likelihood. Our results show that our model is versatile
  and yields tighter mutual information estimates than two intrinsic probes previously
  proposed in the literature. Finally, we find empirical evidence that pre-trained
  representations develop a cross-lingually entangled notion of morphosyntax.
publication: '*Proceedings of the 37th AAAI Conference on Artificial Intelligence*'
links:
- name: URL
  url: https://arxiv.org/abs/2201.08214
url_pdf: https://arxiv.org/pdf/2205.02023.pdf
---
