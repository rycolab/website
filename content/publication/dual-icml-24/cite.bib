@inproceedings{du+al.icml24,
 abstract = {Recent papers have demonstrated the possibility of energy-based text generation by adapting gradient-based sampling algorithms, a paradigm of MCMC algorithms that promises fast convergence. However, as we show in this paper, previous attempts on this approach to text generation all fail to sample correctly from the target language model distributions. To address this limitation, we consider the problem of designing text samplers that are faithful, meaning that they have the target text distribution as its limiting distribution. We propose several faithful gradient-based sampling algorithms to sample from the target energy-based text distribution correctly, and study their theoretical properties. Through experiments on various forms of text generation, we demonstrate that faithful samplers are able to generate more fluent text while adhering to the control objectives better.},
 address = {Vienna, Austria},
 arxiv = {https://arxiv.org/pdf/2312.17710},
 author = {Du, Li and 
Amini, Afra and 
Torroba Hennigen, Lucas and 
Velocity Yu, Xinyan and 
Lee, Holden and 
Eisner, Jason and 
Cotterell, Ryan},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 month = {July},
 publisher = {International Conference on Machine Learning (ICML)},
 slides = {N/A},
 title = {Principled Gradient-Based MCMC for Conditional Sampling of Text},
 url = {https://arxiv.org/pdf/2312.17710},
 venue = {ICML},
 video = {N/A},
 year = {2024}
}
