@article{valvoda+al.tacl23,
 abstract = {Every legal case sets a precedent by developing the law in one of the following two ways. It either expands its scope, in which case it sets positive precedent, or it narrows it down, in which case it sets negative precedent. While legal outcome prediction, which is nothing other than the prediction of positive precedents, is an increasingly popular task in AI, we are the first to investigate negative precedent prediction by focusing on negative outcomes. We discover an asymmetry in existing modelsâ€™ ability to predict positive and negative outcomes. Where state-of-the-art outcome prediction models predicts positive outcomes at 75.06 F1, they predicts negative outcomes at only 10.09 F1, worse than a random baseline. To address this performance gap, we develop two new models inspired by the dynamics of a court process. Our first model significantly improves positive outcome prediction score to 77.15 F1 and our second model more than doubles the negative outcome prediction performance to 24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals that there is still plenty of room to grow when it comes to modelling law.},
 arxiv = {https://arxiv.org/abs/2208.08225},
 author = {Valvoda, Josef and 
Teufel, Simone and 
Cotterell, Ryan},
 booktitle = {Transactions of the Association for Computational Linguistics},
 code = {https://github.com/valvoda/Negative-Precedent-in-Legal-Outcome-Prediction},
 month = {January},
 pages = {34-48},
 publisher = {Association for Computational Linguistics},
 slides = {https://docs.google.com/presentation/d/1q9sca-5lVarkZckDcUMqp_SGCSPlhw5ftfprXiwxQvo/edit?usp=sharing},
 title = {On the Role of Negative Precedent in Legal Outcome Prediction},
 url = {https://arxiv.org/abs/2208.08225},
 venue = {TACL},
 volume = {11},
 year = {2023}
}
