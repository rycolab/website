@inproceedings{ravfogel+al.icml22,
 abstract = {Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to \emphcontrol their content becomes an increasingly important problem. We formulate the problem of identifying and erasing a linear subspace that corresponds to a given concept, in order to prevent linear predictors from recovering the concept. We model this problem as a constrained, linear minimax game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation, R-LACE, that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. We show that the method -- despite being linear -- is highly expressive, effectively mitigating bias in deep nonlinear classifiers while maintaining tractability and interpretability.},
 address = {Baltimore, United States},
 arxiv = {https://arxiv.org/abs/2201.12091},
 author = {Ravfogel, Shauli and 
Twiton, Michael and 
Goldberg, Yoav and 
Cotterell, Ryan},
 booktitle = {Proceedings of the 39th International Conference on Machine Learning},
 code = {https://github.com/shauli-ravfogel/rlace-icml},
 month = {July},
 pages = {18400--18421},
 publisher = {Proceedings of Machine Learning Research},
 slides = {https://docs.google.com/presentation/d/15dwWs3X6NC_3OIE7j6-Yq1-JE9fQDVgvql0WN0EErU0/edit?usp=sharing},
 title = {Linear Adversarial Concept Erasure},
 url = {https://arxiv.org/abs/2201.12091},
 venue = {ICML},
 video = {https://www.youtube.com/watch?v=S9QK2wvXLq0&feature=youtu.be},
 volume = {162},
 year = {2022}
}

