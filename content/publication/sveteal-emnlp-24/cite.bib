@inproceedings{svete+al.emnlp24,
 abstract = {Much theoretical work has described the ability of transformers to represent formal languages. However, linking theoretical results to empirical performance is not straightforward due to the complex interplay between the architecture, the learning algorithm, and training data. To test whether theoretical lower bounds imply learnability of formal languages, we turn to recent work relating transformers to n-gram language models (LMs). We study transformers’ ability to learn random n-gram LMs of two kinds: ones with arbitrary next-symbol probabilities and ones where those are defined with shared parameters. We find that classic estimation techniques for n-gram LMs such as add-λ smoothing outperform transformers on the former, while transformers perform better on the latter, outperforming methods specifically designed to learn n-gram LMs},
 address = {Miami, United States},
 arxiv = {https://arxiv.org/abs/2410.03001},
 author = {Svete, Anej and 
Borenstein, Nadav and 
Zhou, Mike and 
Augenstein, Isabelle and 
Cotterell, Ryan},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
 code = {https://github.com/rycolab/learning-ngrams},
 month = {November},
 publisher = {Association of Computational Linguistics},
 slides = {N/A},
 title = {Can Transformer Language Models Learn $n$-gram Language Models?},
 url = {https://arxiv.org/abs/2410.03001},
 venue = {EMNLP},
 video = {N/A},
 year = {2024}
}
