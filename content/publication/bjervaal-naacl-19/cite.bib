@inproceedings{bjerva+al.naacl19,
 title = {A Probabilistic Generative Model of Linguistic Typology},
 author = {Bjerva, Johannes and 
Kementchedjhieva, Yova and 
Cotterell, Ryan and 
Augenstein, Isabelle},
 booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 month = {June},
 year = {2019},
 address = {Minneapolis, Minnesota},
 publisher = {Association for Computational Linguistics},
 pages = {1529--1540},
 volume = {1 (Long and Short Papers)},
 url = {https://www.aclweb.org/anthology/N19-1156.pdf},
 arxiv = {https://arxiv.org/abs/1903.10950},
 abstract = {In the principles-and-parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features. Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.}
}

