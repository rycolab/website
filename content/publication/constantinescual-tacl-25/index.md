---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Investigating Critical Period Effects in Language Acquisition through Neural
  Language Models
subtitle: ''
summary: ''
authors:
- Ionut Constantinescu
- Tiago Pimentel
- Ryan Cotterell
- Alex Warstadt
tags: []
categories: []
date: '2025-09-01'
lastmod: 2025-07-15T18:14:12+02:00
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2025-07-15T16:17:28.904300Z'
publication_types:
- '2'
abstract: 'Humans appear to have a critical period (CP) for language acquisition:
  Second language (L2) acquisition becomes harder after early childhood, and ceasing
  exposure to a first language (L1) after this period (but not before) typically does
  not lead to substantial loss of L1 proficiency. It is unknown whether these CP effects
  result from innately determined brain maturation or as a stabilization of neural
  connections naturally induced by experience. In this study, we use language models
  (LMs) to test the extent to which these phenomena are peculiar to humans, or shared
  by a broader class of language learners. We vary the age of exposure by training
  LMs on language pairs in various experimental conditions, and find that LMs, which
  lack any direct analog to innate maturational stages, do not show CP effects when
  the age of exposure of L2 is delayed. Our results contradict the claim that CP effects
  are an inevitable result of statistical learning, and they are consistent with an
  innate mechanism for CP effects. We show that we can reverse-engineer the CP by
  introducing a regularizer partway through training to simulate a maturational decrease
  in plasticity. All in all, our results suggest that L1 learning on its own may not
  be enough to induce a CP, and additional engineering is necessary to make language
  models more cognitively plausible.'
publication: '*Transactions of the Association for Computational Linguistics*'
links:
- name: URL
  url: https://arxiv.org/abs/2407.19325
---
