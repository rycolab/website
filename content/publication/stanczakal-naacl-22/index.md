---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained
  Models'
subtitle: ''
summary: ''
authors:
- Karolina Sta≈Ñczak
- Edoardo Ponti
- Lucas Torroba Hennigen
- Ryan Cotterell
- Isabelle Augenstein
tags: []
categories: []
date: '2022-07-01'
lastmod: 2022-11-21T00:29:29+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-07-09T14:51:06.360792Z'
publication_types:
- '1'
abstract: The success of multilingual pre-trained models is underpinned by their ability
  to learn representations shared by multiple languages even in absence of any explicit
  supervision. However, it remains unclear how these models learn to generalise across
  languages. In this work, we conjecture that multilingual pre-trained models can
  derive language-universal abstractions about grammar. In particular, we investigate
  whether morphosyntactic information is encoded in the same subset of neurons in
  different languages.We conduct the first large-scale empirical study over 43 languages
  and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our
  findings show that the cross-lingual overlap between neurons is significant, but
  its extent may vary across categories and depends on language proximity and pre-training
  data size.
publication: '*Proceedings of the 2022 Conference of the North American Chapter of
  the Association for Computational Linguistics: Human Language Technologies*'
links:
- name: URL
  url: https://arxiv.org/abs/2205.02023
url_pdf: https://arxiv.org/pdf/2205.02023.pdf
---
