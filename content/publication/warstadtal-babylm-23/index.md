---
title: 'Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally
  Plausible Corpora'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
- Alex Warstadt
- Aaron Mueller
- Leshem Choshen
- Ethan Wilcox
- Chengxu Zhuang
- Juan Ciro
- Rafael Mosquera
- Bhargavi Paranjabe
- Adina Williams
- Tal Linzen
- Ryan Cotterell
author_notes: []

date: '2023-12-01'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2024-06-01T10:01:10.673216Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types:
- '1'
publication: '*Proceedings of the BabyLM Challenge at the 27th Conference on Computational
  Natural Language Learning*'
publication_short: ''

abstract: 'Children can acquire language from less than 100 million words of input.
  Large language models are far less data-efficient: they typically require 3 or 4
  orders of magnitude more data and still do not perform as well as humans on many
  evaluations. These intensive resource demands limit the ability of researchers to
  train new models and use existing models as developmentally plausible cognitive
  models. The BabyLM Challenge is a communal effort in which participants compete
  to optimize language model training on a fixed data budget. Submissions are compared
  on various evaluation tasks targeting grammatical ability, downstream task performance,
  and generalization. Participants can submit to up to three tracks with progressively
  looser data restrictions. From over 30 submissions, we extract concrete recommendations
  on how best to train data-efficient language models, and on where future efforts
  should (and perhaps should not) focus. The winning submissions using the LTG-BERT
  architecture (Samuel et al., 2023) outperformed models trained on trillions of words.
  Other submissions achieved strong results through training on shorter input sequences
  or training a student model on a pretrained teacher. Curriculum learning attempts,
  which accounted for a large number of submissions, were largely unsuccessful, though
  some showed modest improvements'

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: true

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
---

Add the **full text** or **supplementary notes** for the publication here using Markdown formatting.
