---
# Documentation: https://wowchemy.com/docs/managing-content/

title: A Measure-theoretic Characterization of Tight Language Model
subtitle: ''
summary: ''
authors:
- Li Du
- Lucas Torroba Hennigen
- Tiago Pimentel
- Clara Meister
- Jason Eisner
- Ryan Cotterell
tags: []
categories: []
date: '2023-07-01'
lastmod: 2023-07-09T17:59:38+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2024-02-28T20:04:41.144454Z'
publication_types:
- '1'
abstract: Language modeling, a central task in natural language processing, involves
  estimating a probability distribution over strings. In most cases, the estimated
  distribution sums to 1 over all finite strings. However, in some pathological cases,
  probability mass can “leak” onto the set of infinite sequences. In order to characterize
  the notion of leakage more precisely, this paper offers a measure-theoretic treatment
  of language modeling. We prove that many popular language model families are in
  fact tight, meaning that they will not leak in this sense. We also generalize characterizations
  of tightness proposed in previous works.
publication: '*Proceedings of the 61th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)*'
links:
- name: URL
  url: https://arxiv.org/abs/2212.10502
---
