---
title: On the probability-quality paradox in language generation
date: '2022-05-01'
publishDate: '2022-11-20T23:29:28.286970Z'
authors:
- On the probability-quality paradox in language generation
publication_types:
- '1'
abstract: 'When generating natural language from neural probabilistic models, high
  probability does not always coincide with high quality: It has often been observed
  that mode-seeking decoding methods, i.e., those that produce high-probability text
  under the model, lead to unnatural language. On the other hand, the lower-probability
  text generated by stochastic methods is perceived as more human-like. In this note,
  we offer an explanation for this phenomenon by analyzing language generation through
  an information-theoretic lens. Specifically, we posit that human-like language should
  contain an amount of information (quantified as negative log-probability) that is
  close to the entropy of the distribution over natural strings. Further, we posit
  that language with substantially more (or less) information is undesirable. We provide
  preliminary empirical evidence in favor of this hypothesis; quality ratings of both
  human and machine-generated text—covering multiple tasks and common decoding strategies—suggest
  high-quality text has an information content significantly closer to the entropy
  than we would expect by chance.'
featured: true
publication: '*Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)*'
links:
- name: URL
  url: https://arxiv.org/abs/2203.17217
url_pdf: https://arxiv.org/pdf/2203.17217.pdf
---

