---
title: On the probability-quality paradox in language generation

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
- On the probability-quality paradox in language generation

# Author notes (such as 'Equal Contribution')
author_notes: []

date: '2022-05-01'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2026-02-28T10:54:30.431615Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types:
- '1'

# Publication name and optional abbreviated publication name.
publication: '*Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)*'
publication_short: ''

abstract: 'When generating natural language from neural probabilistic models, high
  probability does not always coincide with high quality: It has often been observed
  that mode-seeking decoding methods, i.e., those that produce high-probability text
  under the model, lead to unnatural language. On the other hand, the lower-probability
  text generated by stochastic methods is perceived as more human-like. In this note,
  we offer an explanation for this phenomenon by analyzing language generation through
  an information-theoretic lens. Specifically, we posit that human-like language should
  contain an amount of information (quantified as negative log-probability) that is
  close to the entropy of the distribution over natural strings. Further, we posit
  that language with substantially more (or less) information is undesirable. We provide
  preliminary empirical evidence in favor of this hypothesis; quality ratings of both
  human and machine-generated text—covering multiple tasks and common decoding strategies—suggest
  high-quality text has an information content significantly closer to the entropy
  than we would expect by chance.'

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
venue: ACL
links:
- name: URL
  url: https://arxiv.org/abs/2203.17217
---

Add the **full text** or **supplementary notes** for the publication here using Markdown formatting.
