@inproceedings{chan+al.neurips24,
 abstract = {Pre-trained language encoders---functions that represent text as vectors---are an integral component of many NLP tasks. We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that a faithful measure of similarity needs to be \emphintrinsic, that is, task-independent, yet still be informative of \emphextrinsic similarity---the performance on downstream tasks. It is common to consider two encoders similar if they are \emphhomotopic, i.e., if they can be aligned through some transformation. In this spirit, we study the properties of \emphaffine alignment of language encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them.},
 address = {Vancouver, Canada},
 arxiv = {https://arxiv.org/abs/2406.02329},
 author = {S. M. Chan, Robin and 
Boumasmoud, Reda and 
Svete, Anej and 
Ren, Yuxin and 
Guo, Qipeng and 
Jin, Zhijing and 
Ravfogel, Shauli and 
Sachan, Mrinmaya and 
Sch√∂lkopf, Bernhard and 
El-Assady, Mennatallah and 
Cotterell, Ryan},
 booktitle = {Advances in Neural Information Processing Systems 38 (2024)},
 code = {https://github.com/chanr0/affine-homotopy},
 month = {December},
 publisher = {Curran Associates, Inc.},
 slides = {N/A},
 title = {On Affine Homotopy between Language Encoders},
 url = {https://arxiv.org/abs/2406.02329},
 venue = {NeurIPS},
 year = {2024}
}
