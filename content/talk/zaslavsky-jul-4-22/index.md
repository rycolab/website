---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "Losing bits and finding meaning: Efficient compression shapes meaning in language"
event: 
event_url:
location: CAB H52
address: 
  street:
  city:
  region:
  postcode:
  country:
summary: "Our world is extremely complex, and yet we are able to exchange our thoughts and beliefs about it using a relatively small number of words. What computational principles can explain this extraordinary ability?"
abstract: "Our world is extremely complex, and yet we are able to exchange our thoughts and beliefs about it using a relatively small number of words. What computational principles can explain this extraordinary ability? In this talk, I argue that in order to communicate and reason about meaning while operating under limited resources, both humans and machines must efficiently compress their representations of the world. In support of this claim, I present a series of studies showing that: (i) languages evolve under pressure to efficiently compress meanings into words; (ii) the same principle can give rise to human-like semantic representations in artificial neural networks trained for vision; and (iii) efficient compression may also explain how meaning is constructed in real time, as interlocutors reason pragmatically about each otherâ€™s intentions and beliefs. Taken together, these results suggest that efficient compression underlies how humans communicate and reason about meaning, and may guide the development of artificial agents that can naturally communicate and collaborate with humans. "
# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: 2022-07-04T14:00:00+02:00
date_end: 2022-07-04T15:00:00+02:00
all_day: false

# Schedule page publish date (NOT event date).
publishDate: 2022-07-19T23:46:48+02:00

authors: ["Noga Zaslavsky"]
tags: []

# Is this a featured event? (true/false)
featured: true

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

# Optional filename of your slides within your event's folder or a URL.
url_slides: 

url_code:
url_pdf: 
url_video:

# Markdown Slides (optional).
#   Associate this event with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides:

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

### Bio
 Noga Zaslavsky is a postdoc at MIT. Her research aims to understand language, learning, and reasoning from first principles, building on ideas and methods from machine learning and information theory.