+++
title = 'Neural Networks and Computational Complexity'
subtitle = 'ETH Zürich: Fall 2024'


active = true  # Activate this widget? true/false
weight = 20
[design]
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns = "1"
[advanced]
 # Custom CSS. 
 css_style = "padding-bottom: 0px;"

+++
## Course Description
This Bachelor’s seminar delves into the fascinating world of modern large language models (LLMs), which have revolutionized natural language processing. As these models continue to evolve and impact various domains, we will explore their potential, limitations, and underlying mechanisms through a theoretical lens. Throughout the seminar, we will address the following key questions: what are the real capabilities of large language models? What are their inherent limitations? How do these models function at a fundamental level? Under what circumstances are they likely to fail? Can we develop a comprehensive "science of LLMs" to address these inquiries? We will leverage formal language theory to provide a rigorous framework for understanding the representational capacity of neural language models.

**Time:** Friday 14-16h

**Location:** CHN D 44

**Additional Material**

[slides](https://docs.google.com/presentation/d/1946fEYNxq1DnsYpg4FtF54EPfl0-NvHICn5XeaTOKGs/edit?usp=sharing)

**Course Schedule (Work in Progress)**

<table class="table">
  <head>
    <base target="_blank">
  </head>
  <thead>
    <tr>
      <th scope="col" style='white-space:nowrap'>Week</th>
      <th scope="col" style='white-space:nowrap'>Date</th>
      <th scope="col" style='white-space:nowrap'>Topic</th>
      <th scope="col" style='white-space:nowrap'>Presenter</th>
      <th scope="col" style='white-space:nowrap'>Reading</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">1</th>
      <td>20.09.24</td>
      <td> Intro </td>
      <td> 
      </td>
      <td>
      </td>
    </tr>  
    <tr>
      <th scope="row">2</th>
      <td>27.09.24</td>
      <td> Language Models & FLT </td>
      <td> Only Lecture
      </td>
      <td>
      </td>
    </tr>  
     <tr>
      <th scope="row">3</th>
      <td>4.10.24</td>
      <td> RNNs and FSAs </td>
      <td> Mary, Jakob, Pierre
      </td>
      <td> <a href=https://aclanthology.org/2024.naacl-long.380/ target="_blank"><b> Svete et al. (2024)</b></a>, <a href=https://aclanthology.org/2024.findings-acl.244/ target="_blank"><b> Svete et al. (2024)</b></a>,
      </td>
    </tr>  
    <tr>
      <th scope="row">4</th>
      <td>11.10.24</td>
      <td> Counter Machines and the LSTM </td>
      <td>
      Tom, Simon, Julius
      </td>
      <td> <a href=https://aclanthology.org/P18-2117.pdf target="_blank"><b> Weiss et al. (2017)</b></a>,
      </td>
    </tr>  
    <tr>
      <th scope="row">5</th>
      <td>18.10.24</td>
      <td> RNNs and Turing Machines </td>
      <td>
      </td>
      <td>
      <a href=https://aclanthology.org/2023.emnlp-main.434v2.pdf target="_blank"><b> Nowak et al. (2023)</b></a>,
      <a href=https://www.sciencedirect.com/science/article/pii/S0022000085710136 target="_blank"><b> Siegelmann and Sontag (1992)</b></a>,
      </td>
    </tr>  
    <tr>
      <th scope="row">6</th>
      <td>25.10.24</td>
      <td> The Transformer </td>
      <td>
      Sarah, Alexander,
      Leon
      </td>
      <td>
      <a href=https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html target="_blank"><b> Vaswani et al. (2017)</b></a>,
      <a href=https://arxiv.org/pdf/1409.0473 target="_blank"><b> Bahdanu et al. (2014)</b></a>,
      </td>
    </tr>  
    <tr>
      <th scope="row">7</th>
      <td>1.11.24</td>
      <td> The Transformer is Turing Complete</td>
      <td>
      </td>
      <td>
      <a href=https://jmlr.org/papers/v22/20-302.html target="_blank"><b> Perez et al. (2017)</b></a>,
      </td>
    </tr>  
    <tr>
      <th scope="row">8</th>
      <td>8.11.24</td>
      <td> The Transformer is Turing (In)Complete </td>
      <td>
      </td>
      <td>
      <a href=https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00306/43545/Theoretical-Limitations-of-Self-Attention-in target="_blank"><b> Hahn (2020)</b></a>
      </td>
    </tr>  
    <tr>
      <th scope="row">9</th>
      <td>15.11.24</td>
      <td> No Lecture EMNLP </td>
      <td>
      </td>
      <td>
      </td>
    </tr>  
    <tr>
      <th scope="row">10</th>
      <td>22.11.24</td>
      <td> The Transformer with Chain of Thought </td>
      <td>
      </td>
      <td>
      <a href=https://arxiv.org/pdf/2310.07923 target="_blank"><b> Merril and Sabharwal (2024)</b></a>,
      </td>
    </tr>  
    <tr>
      <th scope="row">11</th>
      <td>29.11.24</td>
      <td> Circuit Complexity of The Transformer (I) </td>
      <td>
      </td>
      <td>
      </td>
    </tr>  
    <tr>
      <th scope="row">12</th>
      <td>6.12.24</td>
      <td> Circuit Complexity of The Transformer (II) </td>
      <td>
      </td>
      <td>
      </td>
    </tr>  
  </tbody>
</table>



